### Word Embeddings

Word embedding is the process of converting text into numbers. It is the same thing as vectorizing but is specifc to the context of NLP.

>There are many different methods for word embeddding that are usually placed in two separate categories. I will not go too far into detail about every word embedding technique but will list them if you want to read into them further. 





**Count/Statisitcal word embeddings:**


>  These include One-hot Encoding, Bag of words, N-grams vectorization (Count vectorization is the technique where n = 1, TFIDF vectorization,



**Predictive word embeddings**:









> These include Word2vec, Fast text, Glove

**It is important to note that predictive word embedding techniques will generally perform better than frequency-based techniques. However, I will be using a predictive word embedding tehcnique on a later project**


Since this as a project for unsupervised learning, I will not being using this word embedding technique for the assignment.
